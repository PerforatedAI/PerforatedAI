Using device: cuda
Initializing PerforatedAI Dendritic Optimization...
Variable 'dendrite_update_mode' does not exist.  Ignoring set attempt.
Initializing with switch_mode 1
calling convert on ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): Identity()
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=100, bias=True)
) depth 0
calling convert on : ResNet, depth 0
not calling convert on T_destination depth 0
not calling convert on add_module depth 0
not calling convert on apply depth 0
sub is module but in no lists so going deeper: .avgpool
calling convert on AdaptiveAvgPool2d(output_size=(1, 1)) depth 1
calling convert on .avgpool: AdaptiveAvgPool2d, depth 1
not calling convert on T_destination depth 1
not calling convert on add_module depth 1
not calling convert on apply depth 1
not calling convert on bfloat16 depth 1
not calling convert on buffers depth 1
not calling convert on call_super_init depth 1
not calling convert on children depth 1
not calling convert on compile depth 1
not calling convert on cpu depth 1
not calling convert on cuda depth 1
not calling convert on double depth 1
not calling convert on dump_patches depth 1
not calling convert on eval depth 1
not calling convert on extra_repr depth 1
not calling convert on float depth 1
not calling convert on forward depth 1
not calling convert on get_buffer depth 1
not calling convert on get_extra_state depth 1
not calling convert on get_parameter depth 1
not calling convert on get_submodule depth 1
not calling convert on half depth 1
not calling convert on ipu depth 1
not calling convert on load_state_dict depth 1
not calling convert on modules depth 1
not calling convert on mtia depth 1
not calling convert on named_buffers depth 1
not calling convert on named_children depth 1
not calling convert on named_modules depth 1
not calling convert on named_parameters depth 1
not calling convert on output_size depth 1
not calling convert on parameters depth 1
not calling convert on register_backward_hook depth 1
not calling convert on register_buffer depth 1
not calling convert on register_forward_hook depth 1
not calling convert on register_forward_pre_hook depth 1
not calling convert on register_full_backward_hook depth 1
not calling convert on register_full_backward_pre_hook depth 1
not calling convert on register_load_state_dict_post_hook depth 1
not calling convert on register_load_state_dict_pre_hook depth 1
not calling convert on register_module depth 1
not calling convert on register_parameter depth 1
not calling convert on register_state_dict_post_hook depth 1
not calling convert on register_state_dict_pre_hook depth 1
not calling convert on requires_grad_ depth 1
not calling convert on set_extra_state depth 1
not calling convert on set_submodule depth 1
not calling convert on share_memory depth 1
not calling convert on state_dict depth 1
not calling convert on to depth 1
not calling convert on to_empty depth 1
not calling convert on train depth 1
not calling convert on training depth 1
not calling convert on type depth 1
not calling convert on xpu depth 1
not calling convert on zero_grad depth 1
returning from call to: .avgpool
not calling convert on avgpool depth 0
not calling convert on base_width depth 0
not calling convert on bfloat16 depth 0
sub is module but in no lists so going deeper: .bn1
calling convert on BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) depth 1
calling convert on .bn1: BatchNorm2d, depth 1
not calling convert on T_destination depth 1
not calling convert on add_module depth 1
not calling convert on affine depth 1
not calling convert on apply depth 1
not calling convert on bfloat16 depth 1
not calling convert on bias depth 1
not calling convert on buffers depth 1
not calling convert on call_super_init depth 1
not calling convert on children depth 1
not calling convert on compile depth 1
not calling convert on cpu depth 1
not calling convert on cuda depth 1
not calling convert on double depth 1
not calling convert on dump_patches depth 1
not calling convert on eps depth 1
not calling convert on eval depth 1
not calling convert on extra_repr depth 1
not calling convert on float depth 1
not calling convert on forward depth 1
not calling convert on get_buffer depth 1
not calling convert on get_extra_state depth 1
not calling convert on get_parameter depth 1
not calling convert on get_submodule depth 1
not calling convert on half depth 1
not calling convert on ipu depth 1
not calling convert on load_state_dict depth 1
not calling convert on modules depth 1
not calling convert on momentum depth 1
not calling convert on mtia depth 1
not calling convert on named_buffers depth 1
not calling convert on named_children depth 1
not calling convert on named_modules depth 1
not calling convert on named_parameters depth 1
not calling convert on num_batches_tracked depth 1
not calling convert on num_features depth 1
not calling convert on parameters depth 1
not calling convert on register_backward_hook depth 1
not calling convert on register_buffer depth 1
not calling convert on register_forward_hook depth 1
not calling convert on register_forward_pre_hook depth 1
not calling convert on register_full_backward_hook depth 1
not calling convert on register_full_backward_pre_hook depth 1
not calling convert on register_load_state_dict_post_hook depth 1
not calling convert on register_load_state_dict_pre_hook depth 1
not calling convert on register_module depth 1
not calling convert on register_parameter depth 1
not calling convert on register_state_dict_post_hook depth 1
not calling convert on register_state_dict_pre_hook depth 1
not calling convert on requires_grad_ depth 1
not calling convert on reset_parameters depth 1
not calling convert on reset_running_stats depth 1
not calling convert on running_mean depth 1
not calling convert on running_var depth 1
not calling convert on set_extra_state depth 1
not calling convert on set_submodule depth 1
not calling convert on share_memory depth 1
not calling convert on state_dict depth 1
not calling convert on to depth 1
not calling convert on to_empty depth 1
not calling convert on track_running_stats depth 1
not calling convert on train depth 1
not calling convert on training depth 1
not calling convert on type depth 1
not calling convert on weight depth 1
not calling convert on xpu depth 1
not calling convert on zero_grad depth 1
returning from call to: .bn1
potentially found a norm Layer that wont be converted, this is not recommended: .bn1
Set GPA.pc.set_unwrapped_modules_confirmed(True) to skip this next time
Type 'net' + enter to inspect your network and see what the module type containing this layer is.
Then do one of the following:
 - Add the module type to GPA.pc.get_module_names_to_convert() to wrap it entirely
 - If the norm layer is part of a sequential wrap it and the previous layer in a PAISequential
 - If you do not want to add dendrites to this module add the type to GPA.pc.get_module_names_to_track()
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "C:\Users\Pio Godwin\Desktop\Pytorch_dentric\src\train.py", line 169, in <module>
    main()
    ~~~~^^
  File "C:\Users\Pio Godwin\Desktop\Pytorch_dentric\src\train.py", line 135, in main
    model = UPA.initialize_pai(model)
  File "C:\Users\Pio Godwin\Desktop\Pytorch_dentric\PerforatedAI\perforatedai\utils_perforatedai.py", line 82, in initialize_pai
    model = GPA.pai_tracker.initialize(
        model,
    ...<7 lines>...
        zooming_graph=zooming_graph,
    )
  File "C:\Users\Pio Godwin\Desktop\Pytorch_dentric\PerforatedAI\perforatedai\tracker_perforatedai.py", line 2322, in initialize
    model = UPA.convert_network(model)
  File "C:\Users\Pio Godwin\Desktop\Pytorch_dentric\PerforatedAI\perforatedai\utils_perforatedai.py", line 639, in convert_network
    net = convert_module(
        net, 0, "", [], [], PA.PAINeuronModule, PA.TrackedNeuronModule
    )
  File "C:\Users\Pio Godwin\Desktop\Pytorch_dentric\PerforatedAI\perforatedai\utils_perforatedai.py", line 598, in convert_module
    pdb.set_trace()
    ~~~~~~~~~~~~~^^
  File "C:\Users\Pio Godwin\AppData\Local\Programs\Python\Python313\Lib\bdb.py", line 116, in trace_dispatch
    return self.dispatch_opcode(frame, arg)
           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Pio Godwin\AppData\Local\Programs\Python\Python313\Lib\bdb.py", line 215, in dispatch_opcode
    self.user_opcode(frame)
    ~~~~~~~~~~~~~~~~^^^^^^^
  File "C:\Users\Pio Godwin\AppData\Local\Programs\Python\Python313\Lib\pdb.py", line 448, in user_line
    self.interaction(frame, None)
    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^
  File "C:\Users\Pio Godwin\AppData\Local\Programs\Python\Python313\Lib\pdb.py", line 625, in interaction
    self._cmdloop()
    ~~~~~~~~~~~~~^^
  File "C:\Users\Pio Godwin\AppData\Local\Programs\Python\Python313\Lib\pdb.py", line 511, in _cmdloop
    self.cmdloop()
    ~~~~~~~~~~~~^^
  File "C:\Users\Pio Godwin\AppData\Local\Programs\Python\Python313\Lib\cmd.py", line 111, in cmdloop
    if readline.backend == "editline":
       ^^^^^^^^^^^^^^^^
AttributeError: module 'readline' has no attribute 'backend'
