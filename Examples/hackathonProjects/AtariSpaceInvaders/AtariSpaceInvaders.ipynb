{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","authorship_tag":"ABX9TyM9FLHzxVtZfinRZp/O3b64"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install renderlab torch torchvision minatar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jHs_7FY_jwD","executionInfo":{"status":"ok","timestamp":1768891159129,"user_tz":480,"elapsed":5914,"user":{"displayName":"Homero Roman","userId":"11189158305807063250"}},"outputId":"0b89bb66-5841-44e1-f2e1-940f5e1443a0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting renderlab\n","  Downloading renderlab-0.1.20230421184216-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Collecting minatar\n","  Downloading MinAtar-1.0.15-py3-none-any.whl.metadata (685 bytes)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.0.3)\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.2.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.2)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from minatar) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from minatar) (1.4.9)\n","Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from minatar) (3.10.0)\n","Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from minatar) (2.2.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from minatar) (3.3.1)\n","Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from minatar) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.12/dist-packages (from minatar) (2025.2)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from minatar) (1.16.3)\n","Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from minatar) (0.13.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from minatar) (1.17.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->minatar) (1.3.3)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->minatar) (4.61.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->minatar) (25.0)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->minatar) (2025.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (3.1.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (0.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.4.2)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.67.1)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.32.4)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.1.12)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.37.2)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.6.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2026.1.4)\n","Downloading renderlab-0.1.20230421184216-py3-none-any.whl (4.0 kB)\n","Downloading MinAtar-1.0.15-py3-none-any.whl (16 kB)\n","Installing collected packages: renderlab, minatar\n","Successfully installed minatar-1.0.15 renderlab-0.1.20230421184216\n"]}]},{"cell_type":"code","source":["################################################################################################################\n","# Authors:                                                                                                     #\n","# Kenny Young (kjyoung@ualberta.ca)                                                                            #\n","# Tian Tian(ttian@ualberta.ca)                                                                                 #\n","#                                                                                                              #\n","# python3 dqn.py -g <game>                                                                                     #\n","#   -o, --output <directory/file name prefix>                                                                  #\n","#   -v, --verbose: outputs the average returns every 1000 episodes                                             #\n","#   -l, --loadfile <directory/file name of the saved model>                                                    #\n","#   -a, --alpha <number>: step-size parameter                                                                  #\n","#   -s, --save: save model data every 1000 episodes                                                            #\n","#   -r, --replayoff: disable the replay buffer and train on each state transition                              #\n","#   -t, --targetoff: disable the target network                                                                #\n","#                                                                                                              #\n","# References used for this implementation:                                                                     #\n","#   https://pytorch.org/docs/stable/nn.html#                                                                   #\n","#   https://pytorch.org/docs/stable/torch.html                                                                 #\n","#   https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html                                   #\n","################################################################################################################\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","import time\n","\n","import random, numpy, argparse, logging, os\n","\n","from collections import namedtuple\n","from minatar import Environment\n","\n","################################################################################################################\n","# Constants\n","#\n","################################################################################################################\n","BATCH_SIZE = 32\n","REPLAY_BUFFER_SIZE = 100000\n","TARGET_NETWORK_UPDATE_FREQ = 1000\n","TRAINING_FREQ = 1\n","NUM_FRAMES = 5000000\n","FIRST_N_FRAMES = 100000\n","REPLAY_START_SIZE = 5000\n","END_EPSILON = 0.1\n","STEP_SIZE = 0.00025\n","SQUARED_GRAD_MOMENTUM = 0.95\n","MIN_SQUARED_GRAD = 0.01\n","GAMMA = 0.99\n","EPSILON = 1.0\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","################################################################################################################\n","# class QNetwork\n","#\n","# One hidden 2D conv with variable number of input channels.  We use 16 filters, a quarter of the original DQN\n","# paper of 64.  One hidden fully connected linear layer with a quarter of the original DQN paper of 512\n","# rectified units.  Finally, the output layer is a fully connected linear layer with a single output for each\n","# valid action.\n","#\n","################################################################################################################\n","class QNetwork(nn.Module):\n","    def __init__(self, in_channels, num_actions):\n","\n","        super(QNetwork, self).__init__()\n","\n","        # One hidden 2D convolution layer:\n","        #   in_channels: variable\n","        #   out_channels: 16\n","        #   kernel_size: 3 of a 3x3 filter matrix\n","        #   stride: 1\n","        self.conv = nn.Conv2d(in_channels, 16, kernel_size=3, stride=1)\n","\n","        # Final fully connected hidden layer:\n","        #   the number of linear unit depends on the output of the conv\n","        #   the output consist 128 rectified units\n","        def size_linear_unit(size, kernel_size=3, stride=1):\n","            return (size - (kernel_size - 1) - 1) // stride + 1\n","        num_linear_units = size_linear_unit(10) * size_linear_unit(10) * 16\n","        self.fc_hidden = nn.Linear(in_features=num_linear_units, out_features=128)\n","\n","        # Output layer:\n","        self.output = nn.Linear(in_features=128, out_features=num_actions)\n","\n","    # As per implementation instructions according to pytorch, the forward function should be overwritten by all\n","    # subclasses\n","    def forward(self, x):\n","        # Rectified output from the first conv layer\n","        x = f.relu(self.conv(x))\n","\n","        # Rectified output from the final hidden layer\n","        x = f.relu(self.fc_hidden(x.view(x.size(0), -1)))\n","\n","        # Returns the output from the fully-connected linear layer\n","        return self.output(x)\n","\n","\n","###########################################################################################################\n","# class replay_buffer\n","#\n","# A cyclic buffer of a fixed size containing the last N number of recent transitions.  A transition is a\n","# tuple of state, next_state, action, reward, is_terminal.  The boolean is_terminal is used to indicate\n","# whether if the next state is a terminal state or not.\n","#\n","###########################################################################################################\n","transition = namedtuple('transition', 'state, next_state, action, reward, is_terminal')\n","class replay_buffer:\n","    def __init__(self, buffer_size):\n","        self.buffer_size = buffer_size\n","        self.location = 0\n","        self.buffer = []\n","\n","    def add(self, *args):\n","        # Append when the buffer is not full but overwrite when the buffer is full\n","        if len(self.buffer) < self.buffer_size:\n","            self.buffer.append(transition(*args))\n","        else:\n","            self.buffer[self.location] = transition(*args)\n","\n","        # Increment the buffer location\n","        self.location = (self.location + 1) % self.buffer_size\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.buffer, batch_size)\n","\n","\n","################################################################################################################\n","# get_state\n","#\n","# Converts the state given by the environment to a tensor of size (in_channel, 10, 10), and then\n","# unsqueeze to expand along the 0th dimension so the function returns a tensor of size (1, in_channel, 10, 10).\n","#\n","# Input:\n","#   s: current state as numpy array\n","#\n","# Output: current state as tensor, permuted to match expected dimensions\n","#\n","################################################################################################################\n","def get_state(s):\n","    return (torch.tensor(s, device=device).permute(2, 0, 1)).unsqueeze(0).float()\n","\n","\n","################################################################################################################\n","# world_dynamics\n","#\n","# It generates the next state and reward after taking an action according to the behavior policy.  The behavior\n","# policy is epsilon greedy: epsilon probability of selecting a random action and 1 - epsilon probability of\n","# selecting the action with max Q-value.\n","#\n","# Inputs:\n","#   t : frame\n","#   replay_start_size: number of frames before learning starts\n","#   num_actions: number of actions\n","#   s: current state\n","#   env: environment of the game\n","#   policy_net: policy network, an instance of QNetwork\n","#\n","# Output: next state, action, reward, is_terminated\n","#\n","################################################################################################################\n","def world_dynamics(t, replay_start_size, num_actions, s, env, policy_net):\n","\n","    # A uniform random policy is run before the learning starts\n","    if t < replay_start_size:\n","        action = torch.tensor([[random.randrange(num_actions)]], device=device)\n","    else:\n","        # Epsilon-greedy behavior policy for action selection\n","        # Epsilon is annealed linearly from 1.0 to END_EPSILON over the FIRST_N_FRAMES and stays 0.1 for the\n","        # remaining frames\n","        epsilon = END_EPSILON if t - replay_start_size >= FIRST_N_FRAMES \\\n","            else ((END_EPSILON - EPSILON) / FIRST_N_FRAMES) * (t - replay_start_size) + EPSILON\n","\n","        if numpy.random.binomial(1, epsilon) == 1:\n","            action = torch.tensor([[random.randrange(num_actions)]], device=device)\n","        else:\n","            # State is 10x10xchannel, max(1)[1] gives the max action value (i.e., max_{a} Q(s, a)).\n","            # view(1,1) shapes the tensor to be the right form (e.g. tensor([[0]])) without copying the\n","            # underlying tensor.  torch._no_grad() avoids tracking history in autograd.\n","            with torch.no_grad():\n","                action = policy_net(s).max(1)[1].view(1, 1)\n","\n","    # Act according to the action and observe the transition and reward\n","    reward, terminated = env.act(action)\n","\n","    # Obtain s_prime\n","    s_prime = get_state(env.state())\n","\n","    return s_prime, action, torch.tensor([[reward]], device=device).float(), torch.tensor([[terminated]], device=device)\n","\n","\n","################################################################################################################\n","# train\n","#\n","# This is where learning happens. More specifically, this function learns the weights of the policy network\n","# using huber loss.\n","#\n","# Inputs:\n","#   sample: a batch of size 1 or 32 transitions\n","#   policy_net: an instance of QNetwork\n","#   target_net: an instance of QNetwork\n","#   optimizer: centered RMSProp\n","#\n","################################################################################################################\n","def train(sample, policy_net, target_net, optimizer):\n","    # Batch is a list of namedtuple's, the following operation returns samples grouped by keys\n","    batch_samples = transition(*zip(*sample))\n","\n","    # states, next_states are of tensor (BATCH_SIZE, in_channel, 10, 10) - inline with pytorch NCHW format\n","    # actions, rewards, is_terminal are of tensor (BATCH_SIZE, 1)\n","    states = torch.cat(batch_samples.state)\n","    next_states = torch.cat(batch_samples.next_state)\n","    actions = torch.cat(batch_samples.action)\n","    rewards = torch.cat(batch_samples.reward)\n","    is_terminal = torch.cat(batch_samples.is_terminal)\n","\n","    # Obtain a batch of Q(S_t, A_t) and compute the forward pass.\n","    # Note: policy_network output Q-values for all the actions of a state, but all we need is the A_t taken at time t\n","    # in state S_t.  Thus we gather along the columns and get the Q-values corresponds to S_t, A_t.\n","    # Q_s_a is of size (BATCH_SIZE, 1).\n","    Q_s_a = policy_net(states).gather(1, actions)\n","\n","    # Obtain max_{a} Q(S_{t+1}, a) of any non-terminal state S_{t+1}.  If S_{t+1} is terminal, Q(S_{t+1}, A_{t+1}) = 0.\n","    # Note: each row of the network's output corresponds to the actions of S_{t+1}.  max(1)[0] gives the max action\n","    # values in each row (since this a batch).  The detach() detaches the target net's tensor from computation graph so\n","    # to prevent the computation of its gradient automatically.  Q_s_prime_a_prime is of size (BATCH_SIZE, 1).\n","\n","    # Get the indices of next_states that are not terminal\n","    none_terminal_next_state_index = torch.tensor([i for i, is_term in enumerate(is_terminal) if is_term == 0], dtype=torch.int64, device=device)\n","    # Select the indices of each row\n","    none_terminal_next_states = next_states.index_select(0, none_terminal_next_state_index)\n","\n","    Q_s_prime_a_prime = torch.zeros(len(sample), 1, device=device)\n","    if len(none_terminal_next_states) != 0:\n","        Q_s_prime_a_prime[none_terminal_next_state_index] = target_net(none_terminal_next_states).detach().max(1)[0].unsqueeze(1)\n","\n","    # Compute the target\n","    target = rewards + GAMMA * Q_s_prime_a_prime\n","\n","    # Huber loss\n","    loss = f.smooth_l1_loss(target, Q_s_a)\n","\n","    # Zero gradients, backprop, update the weights of policy_net\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","################################################################################################################\n","# dqn\n","#\n","# DQN algorithm with the option to disable replay and/or target network, and the function saves the training data.\n","#\n","# Inputs:\n","#   env: environment of the game\n","#   replay_off: disable the replay buffer and train on each state transition\n","#   target_off: disable target network\n","#   output_file_name: directory and file name prefix to output data and network weights, file saved as\n","#       <output_file_name>_data_and_weights\n","#   store_intermediate_result: a boolean, if set to true will store checkpoint data every 1000 episodes\n","#       to a file named <output_file_name>_checkpoint\n","#   load_path: file path for a checkpoint to load, and continue training from\n","#   step_size: step-size for RMSProp optimizer\n","#\n","#################################################################################################################\n","def dqn(env, replay_off, target_off, output_file_name, store_intermediate_result=False, load_path=None, step_size=STEP_SIZE):\n","\n","    # Get channels and number of actions specific to each game\n","    in_channels = env.state_shape()[2]\n","    num_actions = env.num_actions()\n","\n","    # Instantiate networks, optimizer, loss and buffer\n","    policy_net = QNetwork(in_channels, num_actions).to(device)\n","    replay_start_size = 0\n","    if not target_off:\n","        target_net = QNetwork(in_channels, num_actions).to(device)\n","        target_net.load_state_dict(policy_net.state_dict())\n","\n","    if not replay_off:\n","        r_buffer = replay_buffer(REPLAY_BUFFER_SIZE)\n","        replay_start_size = REPLAY_START_SIZE\n","\n","    optimizer = optim.RMSprop(policy_net.parameters(), lr=step_size, alpha=SQUARED_GRAD_MOMENTUM, centered=True, eps=MIN_SQUARED_GRAD)\n","\n","    # Set initial values\n","    e_init = 0\n","    t_init = 0\n","    policy_net_update_counter_init = 0\n","    avg_return_init = 0.0\n","    data_return_init = []\n","    frame_stamp_init = []\n","\n","    # Load model and optimizer if load_path is not None\n","    if load_path is not None and isinstance(load_path, str):\n","        checkpoint = torch.load(load_path)\n","        policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n","\n","        if not target_off:\n","            target_net.load_state_dict(checkpoint['target_net_state_dict'])\n","\n","        if not replay_off:\n","            r_buffer = checkpoint['replay_buffer']\n","\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        e_init = checkpoint['episode']\n","        t_init = checkpoint['frame']\n","        policy_net_update_counter_init = checkpoint['policy_net_update_counter']\n","        avg_return_init = checkpoint['avg_return']\n","        data_return_init = checkpoint['return_per_run']\n","        frame_stamp_init = checkpoint['frame_stamp_per_run']\n","\n","        # Set to training mode\n","        policy_net.train()\n","        if not target_off:\n","            target_net.train()\n","\n","    # Data containers for performance measure and model related data\n","    data_return = data_return_init\n","    frame_stamp = frame_stamp_init\n","    avg_return = avg_return_init\n","\n","    # Train for a number of frames\n","    t = t_init\n","    e = e_init\n","    policy_net_update_counter = policy_net_update_counter_init\n","    t_start = time.time()\n","    while t < NUM_FRAMES:\n","        # Initialize the return for every episode (we should see this eventually increase)\n","        G = 0.0\n","\n","        # Initialize the environment and start state\n","        env.reset()\n","        s = get_state(env.state())\n","        is_terminated = False\n","        while(not is_terminated) and t < NUM_FRAMES:\n","            # Generate data\n","            s_prime, action, reward, is_terminated = world_dynamics(t, replay_start_size, num_actions, s, env, policy_net)\n","\n","            sample = None\n","            if replay_off:\n","                sample = [transition(s, s_prime, action, reward, is_terminated)]\n","            else:\n","                # Write the current frame to replay buffer\n","                r_buffer.add(s, s_prime, action, reward, is_terminated)\n","\n","                # Start learning when there's enough data and when we can sample a batch of size BATCH_SIZE\n","                if t > REPLAY_START_SIZE and len(r_buffer.buffer) >= BATCH_SIZE:\n","                    # Sample a batch\n","                    sample = r_buffer.sample(BATCH_SIZE)\n","\n","            # Train every n number of frames defined by TRAINING_FREQ\n","            if t % TRAINING_FREQ == 0 and sample is not None:\n","                if target_off:\n","                    train(sample, policy_net, policy_net, optimizer)\n","                else:\n","                    policy_net_update_counter += 1\n","                    train(sample, policy_net, target_net, optimizer)\n","\n","            # Update the target network only after some number of policy network updates\n","            if not target_off and policy_net_update_counter > 0 and policy_net_update_counter % TARGET_NETWORK_UPDATE_FREQ == 0:\n","                target_net.load_state_dict(policy_net.state_dict())\n","\n","            G += reward.item()\n","\n","            t += 1\n","\n","            # Continue the process\n","            s = s_prime\n","\n","        # Increment the episodes\n","        e += 1\n","\n","        # Save the return for each episode\n","        data_return.append(G)\n","        frame_stamp.append(t)\n","\n","        # Logging exponentiated return only when verbose is turned on and only at 1000 episode intervals\n","        avg_return = 0.99 * avg_return + 0.01 * G\n","        if e % 1000 == 0:\n","            logging.info(\"Episode \" + str(e) + \" | Return: \" + str(G) + \" | Avg return: \" +\n","                         str(numpy.around(avg_return, 2)) + \" | Frame: \" + str(t)+\" | Time per frame: \" +str((time.time()-t_start)/t) )\n","\n","        # Save model data and other intermediate data if the corresponding flag is true\n","        if store_intermediate_result and e % 1000 == 0:\n","            torch.save({\n","                        'episode': e,\n","                        'frame': t,\n","                        'policy_net_update_counter': policy_net_update_counter,\n","                        'policy_net_state_dict': policy_net.state_dict(),\n","                        'target_net_state_dict': target_net.state_dict() if not target_off else [],\n","                        'optimizer_state_dict': optimizer.state_dict(),\n","                        'avg_return': avg_return,\n","                        'return_per_run': data_return,\n","                        'frame_stamp_per_run': frame_stamp,\n","                        'replay_buffer': r_buffer if not replay_off else []\n","            }, output_file_name + \"_checkpoint\")\n","\n","    # Print final logging info\n","    logging.info(\"Avg return: \" + str(numpy.around(avg_return, 2)) + \" | Time per frame: \" + str((time.time()-t_start)/t))\n","\n","    # Write data to file\n","    torch.save({\n","        'returns': data_return,\n","        'frame_stamps': frame_stamp,\n","        'policy_net_state_dict': policy_net.state_dict()\n","    }, output_file_name + \"_data_and_weights\")\n","\n","\n","def main(args):\n","    if args.verbose:\n","        logging.basicConfig(level=logging.INFO)\n","\n","    # If there's an output specified, then use the user specified output.  Otherwise, create file in the current\n","    # directory with the game's name.\n","    if args.output:\n","        file_name = args.output\n","    else:\n","        file_name = os.getcwd() + \"/\" + args.game\n","\n","    load_file_path = None\n","    if args.loadfile:\n","        load_file_path = args.loadfile\n","\n","    env = Environment(args.game)\n","\n","    print('Cuda available?: ' + str(torch.cuda.is_available()))\n","    dqn(env, args.replayoff, args.targetoff, file_name, args.save, load_file_path, args.alpha)"],"metadata":{"id":"NaNPYnv7AfQ0","executionInfo":{"status":"ok","timestamp":1768891166779,"user_tz":480,"elapsed":5635,"user":{"displayName":"Homero Roman","userId":"11189158305807063250"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# parser = argparse.ArgumentParser()\n","# parser.add_argument(, \"-g\", type=str)\n","# parser.add_argument(, \"-o\", type=str)\n","# parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\")\n","# parser.add_argument(\"--loadfile\", \"-l\", type=str)\n","# parser.add_argument(\"--alpha\", \"-a\", type=float, default=)\n","# parser.add_argument(\"--save\", \"-s\", action=\"store_true\")\n","# parser.add_argument(\"--replayoff\", \"-r\", action=\"store_true\")\n","# parser.add_argument(\"--targetoff\", \"-t\", action=\"store_true\")\n","class ARGS:\n","  game = \"space_invaders\"\n","  output = \"\"\n","  verbose = True\n","  loadfile = \"\"\n","  alpha = STEP_SIZE\n","  save = False\n","  replayoff = False\n","  targetoff = False\n","\n","main(ARGS())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bkCsNmfTBDIm","outputId":"3def5582-11af-4275-b366-b3c166c1aa81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda available?: True\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"X30Og4ZNCi5i"},"execution_count":null,"outputs":[]}]}