# -*- coding: utf-8 -*-
"""Dentrites_Hackathon_STS-B (Semantic Textual Similarity Benchmark).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c_x3jk0Ywzwc-adIa_YCWVneWYwxquTR

## W&B Key - "4d4d4c8f520ba4f560db3da3a2e051523a812c46"
"""

!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/PerforatedAI/PerforatedAI.git
# %cd PerforatedAI

# install
!pip install -e .

# Install dependencies
!pip install -q transformers datasets accelerate wandb

"""## CIFAR-10 dataset start with new basline and dentrics"""

# =========================
# CELL 1 — Install + Imports
# =========================
!pip -q install -U sentence-transformers datasets wandb
!pip -q install -e .

import os, time
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

import wandb
from datasets import load_dataset
from sentence_transformers import SentenceTransformer

from perforatedai import globals_perforatedai as GPA
from perforatedai import utils_perforatedai as UPA

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("device:", device)

WANDB_PROJECT = "perforatedai-sbert-stsb"

# Disable ipdb/pdb breaks
os.environ["PYTHONBREAKPOINT"] = "0"

# =========================
# CELL 2 — Load STS-B
# =========================
ds = load_dataset("glue", "stsb")

def normalize_label(x):  # 0..5 -> 0..1
    return float(x) / 5.0

train_data = ds["train"]
val_data   = ds["validation"]

BATCH_SIZE = 32
MAX_LEN = 128

print("train:", len(train_data), "val:", len(val_data))

# ============================================
# CELL 3 — Build SBERT HF Backbone + Tokenizer
# ============================================
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

def build_sbert_encoder():
    sbert = SentenceTransformer(MODEL_NAME, device=device)
    hf_model = sbert._first_module().auto_model
    tokenizer = sbert.tokenizer
    return hf_model, tokenizer

base_model, tokenizer = build_sbert_encoder()
base_model = base_model.to(device)
print("HF model type:", type(base_model))

# =========================
# CELL 4 — Pooling + Dataloaders
# =========================
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state  # [B,T,H]
    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    summed = torch.sum(token_embeddings * mask, dim=1)
    denom = torch.clamp(mask.sum(dim=1), min=1e-9)
    return summed / denom

def collate_fn(batch):
    s1 = [b["sentence1"] for b in batch]
    s2 = [b["sentence2"] for b in batch]
    y  = torch.tensor([normalize_label(b["label"]) for b in batch], dtype=torch.float32)

    t1 = tokenizer(s1, padding=True, truncation=True, max_length=MAX_LEN, return_tensors="pt")
    t2 = tokenizer(s2, padding=True, truncation=True, max_length=MAX_LEN, return_tensors="pt")

    return {"t1": dict(t1), "t2": dict(t2), "labels": y}

train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)
val_loader   = DataLoader(val_data,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)

# =========================
# CELL 5 — Loss + Spearman Eval
# =========================
def cosine_mse_loss(sim, y):
    return F.mse_loss(sim, y)

@torch.no_grad()
def evaluate_spearman(model):
    model.eval()
    preds, golds = [], []

    for batch in val_loader:
        t1 = {k: v.to(device) for k, v in batch["t1"].items()}
        t2 = {k: v.to(device) for k, v in batch["t2"].items()}
        y  = batch["labels"].cpu().numpy()

        out1 = model(**t1)
        out2 = model(**t2)

        emb1 = mean_pooling(out1, t1["attention_mask"])
        emb2 = mean_pooling(out2, t2["attention_mask"])
        emb1 = F.normalize(emb1, p=2, dim=1)
        emb2 = F.normalize(emb2, p=2, dim=1)

        sim = (emb1 * emb2).sum(dim=1).detach().cpu().numpy()

        preds.extend(sim.tolist())
        golds.extend(y.tolist())

    preds = np.array(preds)
    golds = np.array(golds)

    # simple Spearman (no scipy)
    def rankdata(a):
        temp = a.argsort()
        ranks = np.empty_like(temp)
        ranks[temp] = np.arange(len(a))
        return ranks

    rp = rankdata(preds)
    rg = rankdata(golds)
    cov = np.cov(rp, rg, bias=True)[0, 1]
    return float(cov / (rp.std() * rg.std() + 1e-9))

# ===================================================
# CELL 6 — PerforatedAI Fix: this_output_dimensions
# ===================================================
def fix_transformer_output_dims(model, device):
    """
    Transformer encoder Linear layers output [B, T, H].
    Tell PAI neuron dim = last dim (H) -> [-1, -1, 0].
    """
    vec = torch.tensor([-1, -1, 0], device=device)

    if hasattr(model, "encoder") and hasattr(model.encoder, "layer"):
        for layer in model.encoder.layer:
            # attention output dense
            if hasattr(layer, "attention"):
                if hasattr(layer.attention, "output") and hasattr(layer.attention.output, "dense"):
                    layer.attention.output.dense.set_this_output_dimensions(vec)

                # Q/K/V
                if hasattr(layer.attention, "self"):
                    if hasattr(layer.attention.self, "query"):
                        layer.attention.self.query.set_this_output_dimensions(vec)
                    if hasattr(layer.attention.self, "key"):
                        layer.attention.self.key.set_this_output_dimensions(vec)
                    if hasattr(layer.attention.self, "value"):
                        layer.attention.self.value.set_this_output_dimensions(vec)

            # FFN intermediate + output
            if hasattr(layer, "intermediate") and hasattr(layer.intermediate, "dense"):
                layer.intermediate.dense.set_this_output_dimensions(vec)

            if hasattr(layer, "output") and hasattr(layer.output, "dense"):
                layer.output.dense.set_this_output_dimensions(vec)

    return model

# ============================================
# CELL 7 — Baseline training + history (FIXED)
# ============================================
import os
import wandb

# Option 1 (recommended in notebooks): set key once via env
os.environ["WANDB_API_KEY"] = "4d4d4c8f520ba4f560db3da3a2e051523a812c46"
wandb.login()  # safe to call multiple times

baseline_hist = {"epoch": [], "train_loss": [], "val_spearman": []}

def train_baseline(model, lr=2e-5, epochs=1, run_name="baseline_no_dendrites"):
    wandb.init(project=WANDB_PROJECT, name=run_name, reinit="finish_previous")

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)

    for epoch in range(epochs):
        model.train()
        total_loss = 0.0

        for batch in train_loader:
            t1 = {k: v.to(device) for k, v in batch["t1"].items()}
            t2 = {k: v.to(device) for k, v in batch["t2"].items()}
            y  = batch["labels"].to(device)

            out1 = model(**t1)
            out2 = model(**t2)

            emb1 = mean_pooling(out1, t1["attention_mask"])
            emb2 = mean_pooling(out2, t2["attention_mask"])
            emb1 = F.normalize(emb1, p=2, dim=1)
            emb2 = F.normalize(emb2, p=2, dim=1)

            sim = (emb1 * emb2).sum(dim=1)
            loss = cosine_mse_loss(sim, y)

            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()

        spearman = evaluate_spearman(model)

        baseline_hist["epoch"].append(epoch)
        baseline_hist["train_loss"].append(total_loss / len(train_loader))
        baseline_hist["val_spearman"].append(spearman)

        wandb.log({
            "epoch": epoch,
            "train/loss": total_loss / len(train_loader),
            "val/spearman": spearman
        })

        print(f"[Baseline] epoch={epoch} loss={total_loss/len(train_loader):.4f} spearman={spearman:.4f}")

    wandb.finish()
    return model

baseline_model, _ = build_sbert_encoder()
baseline_model = baseline_model.to(device)

baseline_model = train_baseline(baseline_model, lr=2e-5, epochs=3)
print("Baseline Spearman:", evaluate_spearman(baseline_model))

# =========================================================
# CELL 8 — Dendritic training + history (NO PROMPTS)
# =========================================================
dend_hist = {
    "epoch": [],
    "train_loss": [],
    "val_spearman": [],
    "params": [],
    "restructured_epochs": []
}

def train_dendritic(model, lr=2e-5, run_name="dendritic_with_pai",
                    safety_max_epochs=200, warmup_epochs=1):

    wandb.init(project=WANDB_PROJECT, name=run_name, reinit="finish_previous")

    GPA.pc.set_unwrapped_modules_confirmed(True)
    GPA.pc.set_testing_dendrite_capacity(False)

    # IMPORTANT: pooler not used by mean pooling -> track only (do NOT convert)
    GPA.pc.append_module_ids_to_track([".pooler.dense"])

    # Accept weight-decay guidance so PAI doesn't stop for interactive confirmation
    GPA.pc.set_weight_decay_accepted(True)

    # Convert model to PAI (must be before .to(device))
    model = UPA.initialize_pai(model)
    model = fix_transformer_output_dims(model, device)
    model = model.to(device)

    # PAI-managed optimizer/scheduler
    GPA.pai_tracker.set_optimizer(torch.optim.AdamW)
    GPA.pai_tracker.set_scheduler(torch.optim.lr_scheduler.ReduceLROnPlateau)

    # IMPORTANT: do NOT pass weight_decay at all
    optim_args = {"params": model.parameters(), "lr": lr}
    sched_args = {"mode": "max", "patience": 3, "factor": 0.5}

    optimizer, scheduler = GPA.pai_tracker.setup_optimizer(model, optim_args, sched_args)

    epoch = 0
    while True:
        model.train()
        total_loss = 0.0

        for batch in train_loader:
            t1 = {k: v.to(device) for k, v in batch["t1"].items()}
            t2 = {k: v.to(device) for k, v in batch["t2"].items()}
            y  = batch["labels"].to(device)

            out1 = model(**t1)
            out2 = model(**t2)

            emb1 = mean_pooling(out1, t1["attention_mask"])
            emb2 = mean_pooling(out2, t2["attention_mask"])
            emb1 = F.normalize(emb1, p=2, dim=1)
            emb2 = F.normalize(emb2, p=2, dim=1)

            sim = (emb1 * emb2).sum(dim=1)
            loss = cosine_mse_loss(sim, y)

            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()

        spearman = evaluate_spearman(model)

        # Warmup: do normal training for a bit before letting PAI restructure
        if epoch >= warmup_epochs:
            model, restructured, training_complete = GPA.pai_tracker.add_validation_score(spearman, model)
            model = model.to(device)

            if restructured:
                optimizer, scheduler = GPA.pai_tracker.setup_optimizer(model, optim_args, sched_args)
        else:
            restructured = False
            training_complete = False

        # history
        dend_hist["epoch"].append(epoch)
        dend_hist["train_loss"].append(total_loss / len(train_loader))
        dend_hist["val_spearman"].append(spearman)
        dend_hist["params"].append(sum(p.numel() for p in model.parameters()))
        if restructured:
            dend_hist["restructured_epochs"].append(epoch)

        wandb.log({
            "epoch": epoch,
            "train/loss": total_loss / len(train_loader),
            "val/spearman": spearman,
            "pai/restructured": int(restructured),
            "pai/training_complete": int(training_complete),
            "pai/params": sum(p.numel() for p in model.parameters()),
        })

        print(f"[Dendritic] epoch={epoch} loss={total_loss/len(train_loader):.4f} "
              f"spearman={spearman:.4f} restructured={restructured} complete={training_complete}")

        if training_complete:
            print("✅ PAI ended training (training_complete=True).")
            break

        epoch += 1
        if epoch >= safety_max_epochs:
            print("⚠️ Safety stop hit.")
            break

    wandb.finish()
    return model

dend_model, _ = build_sbert_encoder()
dend_model = dend_model.to(device)

dend_model = train_dendritic(dend_model, lr=2e-5, safety_max_epochs=200, warmup_epochs=1)
print("Dendritic Spearman:", evaluate_spearman(dend_model))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.pyplot as plt

#   dend_hist["epoch"], dend_hist["val_spearman"], dend_hist["restructured_epochs"]
#   baseline_hist["epoch"], baseline_hist["val_spearman"]

fig, ax = plt.subplots(figsize=(10, 6))

# With dendrites
ax.plot(dend_hist["epoch"], dend_hist["val_spearman"], label="Validation Spearman (with dendrites)")

# No dendrites baseline
ax.plot(baseline_hist["epoch"], baseline_hist["val_spearman"], linestyle="--",
        label="Validation Spearman (no dendrites)")

# Vertical bars where dendrites were added/absorbed
for e in dend_hist.get("restructured_epochs", []):
    ax.axvline(x=e, linestyle="--")

ax.set_xlabel("Epoch")
ax.set_ylabel("Spearman")
ax.set_title("PAI-style Graph: Spearman vs Epoch (Dendrites Added Marked)")
ax.legend()
ax.grid(True)

plt.show()

