{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6add971d",
   "metadata": {},
   "source": [
    "# üß† Dendritic YOLOv8: PerforatedAI Hackathon\n",
    "\n",
    "**Clean, reproducible notebook** for the PyTorch Dendritic Optimization Hackathon.\n",
    "\n",
    "## What This Demonstrates\n",
    "- Apply PerforatedAI dendritic optimization to YOLOv8n\n",
    "- Compare baseline vs optimized model (params, speed, mAP)\n",
    "- Edge deployment benefits: smaller, faster models\n",
    "\n",
    "## Prerequisites\n",
    "1. **Google Colab with GPU**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "2. **Run cells in order** from top to bottom\n",
    "3. **Fresh runtime recommended**: Runtime ‚Üí Restart runtime before starting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf60ba",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Environment Setup (CRITICAL)\n",
    "\n",
    "**Pin PyTorch < 2.6** to avoid `weights_only` unpickling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa029eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLEAN ENVIRONMENT SETUP ===\n",
    "# Pin PyTorch < 2.6 to avoid weights_only=True errors with YOLO checkpoints\n",
    "\n",
    "!pip -q uninstall -y ultralytics wandb perforatedai || true\n",
    "\n",
    "# Install known-good versions (PyTorch 2.4.1 works well with Colab GPU)\n",
    "!pip -q install \"torch==2.4.1\" \"torchvision==0.19.1\" \"torchaudio==2.4.1\"\n",
    "!pip -q install \"ultralytics==8.2.0\" \"wandb\" \"matplotlib\" \"pandas\" \"seaborn\"\n",
    "!pip -q install \"perforatedai==3.0.7\"\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")\n",
    "\n",
    "# Verify versions\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e458295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEVICE SETUP ===\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üéØ Using device: {device}\")\n",
    "\n",
    "if device == 'cpu':\n",
    "    print(\"‚ö†Ô∏è WARNING: Running on CPU. For best results:\")\n",
    "    print(\"   Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "    print(\"   Then restart runtime and re-run from Cell 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3244bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SANITY CHECK: YOLO loads without errors ===\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"Loading YOLOv8n...\")\n",
    "test_model = YOLO(\"yolov8n.pt\")\n",
    "print(\"‚úÖ YOLO model loads successfully!\")\n",
    "\n",
    "# Quick validation to confirm everything works\n",
    "print(\"\\nRunning quick validation on COCO128...\")\n",
    "metrics = test_model.val(data=\"coco128.yaml\", imgsz=640, device=device, verbose=False)\n",
    "print(f\"‚úÖ Validation works! mAP50-95: {metrics.box.map:.4f}\")\n",
    "\n",
    "del test_model  # Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bca678",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Imports & W&B Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# PerforatedAI imports\n",
    "try:\n",
    "    from perforatedai import globals_perforatedai as GPA\n",
    "    from perforatedai import utils_perforatedai as UPA\n",
    "    PERFORATED_AI_AVAILABLE = True\n",
    "    print(\"‚úÖ PerforatedAI imported successfully!\")\n",
    "except ImportError as e:\n",
    "    PERFORATED_AI_AVAILABLE = False\n",
    "    print(f\"‚ö†Ô∏è PerforatedAI not available: {e}\")\n",
    "\n",
    "print(f\"\\nüì¶ PerforatedAI: {'Available' if PERFORATED_AI_AVAILABLE else 'Not Available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === W&B LOGIN ===\n",
    "# Option 1: Set environment variable WANDB_API_KEY before running\n",
    "# Option 2: Enter API key when prompted\n",
    "# Option 3: Run offline (set WANDB_MODE=offline)\n",
    "\n",
    "try:\n",
    "    wandb.login()\n",
    "    print(\"‚úÖ W&B authenticated!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è W&B login failed: {e}\")\n",
    "    print(\"Running in offline mode...\")\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707b61c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HELPER FUNCTIONS ===\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total and trainable parameters.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def measure_inference_speed(model, img_size=640, num_runs=50, device='cuda'):\n",
    "    \"\"\"Measure average inference time in milliseconds.\"\"\"\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, img_size, img_size).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    return (end - start) / num_runs * 1000\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4112324",
   "metadata": {},
   "source": [
    "---\n",
    "## Section A: Baseline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452081e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BASELINE: Load Model ===\n",
    "print(\"üöÄ Loading YOLOv8n baseline model...\")\n",
    "\n",
    "baseline_model = YOLO(\"yolov8n.pt\")\n",
    "baseline_model.model = baseline_model.model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "baseline_params, _ = count_parameters(baseline_model.model)\n",
    "print(f\"üìä Baseline Parameters: {baseline_params / 1e6:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44326cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BASELINE: Train ===\n",
    "print(\"üèãÔ∏è Training baseline model (5 epochs)...\")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Dendritic-YOLOv8-Hackathon\",\n",
    "    name=\"baseline-yolov8n\",\n",
    "    tags=[\"baseline\", \"yolov8n\", \"coco128\"]\n",
    ")\n",
    "\n",
    "baseline_results = baseline_model.train(\n",
    "    data=\"coco128.yaml\",\n",
    "    epochs=5,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=device,\n",
    "    project=\"runs/baseline\",\n",
    "    name=\"yolov8n_baseline\",\n",
    "    exist_ok=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Baseline training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e99c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BASELINE: Validate & Collect Metrics ===\n",
    "print(\"üìä Validating baseline model...\")\n",
    "\n",
    "baseline_val = baseline_model.val(data=\"coco128.yaml\", device=device)\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"params_M\": baseline_params / 1e6,\n",
    "    \"mAP50\": float(baseline_val.box.map50),\n",
    "    \"mAP50-95\": float(baseline_val.box.map),\n",
    "    \"precision\": float(baseline_val.box.mp),\n",
    "    \"recall\": float(baseline_val.box.mr),\n",
    "    \"inference_ms\": measure_inference_speed(baseline_model.model, device=device)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä BASELINE METRICS:\")\n",
    "for k, v in baseline_metrics.items():\n",
    "    print(f\"   {k}: {v:.4f}\")\n",
    "\n",
    "wandb.log({f\"baseline_{k}\": v for k, v in baseline_metrics.items()})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b1aeb",
   "metadata": {},
   "source": [
    "---\n",
    "## Section B: Dendritic Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DENDRITIC: Load Fresh Model ===\n",
    "print(\"üß† Loading fresh YOLOv8n for dendritic optimization...\")\n",
    "\n",
    "dendritic_yolo = YOLO(\"yolov8n.pt\")\n",
    "dendritic_model = dendritic_yolo.model.to(device)\n",
    "\n",
    "print(f\"üìä Pre-optimization Parameters: {count_parameters(dendritic_model)[0] / 1e6:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7459a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DENDRITIC: Apply PerforatedAI Optimization ===\n",
    "print(\"üß† Applying dendritic optimization...\")\n",
    "\n",
    "if PERFORATED_AI_AVAILABLE:\n",
    "    # Configure PerforatedAI\n",
    "    GPA.pc.set_testing_dendrite_capacity(False)\n",
    "    GPA.pc.set_verbose(True)\n",
    "    GPA.pc.set_dendrite_update_mode(True)\n",
    "    \n",
    "    # Save input stem (model[0]) before optimization to avoid weight mismatch\n",
    "    input_stem = dendritic_model.model[0]\n",
    "    \n",
    "    # Apply dendritic optimization\n",
    "    dendritic_model = UPA.initialize_pai(\n",
    "        dendritic_model,\n",
    "        doing_pai=True,\n",
    "        save_name=\"DendriticYOLOv8\",\n",
    "        maximizing_score=True\n",
    "    )\n",
    "    \n",
    "    # Restore input stem\n",
    "    dendritic_model.model[0] = input_stem\n",
    "    dendritic_model = dendritic_model.to(device)\n",
    "    \n",
    "    print(\"‚úÖ Dendritic optimization applied!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PerforatedAI not available - using standard model\")\n",
    "\n",
    "dendritic_params, _ = count_parameters(dendritic_model)\n",
    "print(f\"üìä Post-optimization Parameters: {dendritic_params / 1e6:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DENDRITIC: Train ===\n",
    "print(\"üèãÔ∏è Training dendritic model (5 epochs)...\")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Dendritic-YOLOv8-Hackathon\",\n",
    "    name=\"dendritic-yolov8n\",\n",
    "    tags=[\"dendritic\", \"perforatedai\", \"yolov8n\", \"coco128\"]\n",
    ")\n",
    "\n",
    "# Reassign modified model to YOLO wrapper\n",
    "dendritic_yolo.model = dendritic_model\n",
    "\n",
    "dendritic_results = dendritic_yolo.train(\n",
    "    data=\"coco128.yaml\",\n",
    "    epochs=5,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=device,\n",
    "    project=\"runs/dendritic\",\n",
    "    name=\"yolov8n_dendritic\",\n",
    "    exist_ok=True,\n",
    "    verbose=True,\n",
    "    optimizer=\"Adam\",\n",
    "    lr0=0.001\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dendritic training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed077ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DENDRITIC: Validate & Collect Metrics ===\n",
    "print(\"üìä Validating dendritic model...\")\n",
    "\n",
    "dendritic_val = dendritic_yolo.val(data=\"coco128.yaml\", device=device)\n",
    "\n",
    "dendritic_metrics = {\n",
    "    \"params_M\": dendritic_params / 1e6,\n",
    "    \"mAP50\": float(dendritic_val.box.map50),\n",
    "    \"mAP50-95\": float(dendritic_val.box.map),\n",
    "    \"precision\": float(dendritic_val.box.mp),\n",
    "    \"recall\": float(dendritic_val.box.mr),\n",
    "    \"inference_ms\": measure_inference_speed(dendritic_yolo.model, device=device)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä DENDRITIC METRICS:\")\n",
    "for k, v in dendritic_metrics.items():\n",
    "    print(f\"   {k}: {v:.4f}\")\n",
    "\n",
    "wandb.log({f\"dendritic_{k}\": v for k, v in dendritic_metrics.items()})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975b0cc",
   "metadata": {},
   "source": [
    "---\n",
    "## Section C: Comparison & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPARISON ===\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä BASELINE vs DENDRITIC COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate deltas\n",
    "param_reduction = ((baseline_metrics['params_M'] - dendritic_metrics['params_M']) / baseline_metrics['params_M']) * 100\n",
    "map50_change = dendritic_metrics['mAP50'] - baseline_metrics['mAP50']\n",
    "map_change = dendritic_metrics['mAP50-95'] - baseline_metrics['mAP50-95']\n",
    "speed_improvement = ((baseline_metrics['inference_ms'] - dendritic_metrics['inference_ms']) / baseline_metrics['inference_ms']) * 100\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Baseline':>12} {'Dendritic':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Parameters (M)':<20} {baseline_metrics['params_M']:>12.3f} {dendritic_metrics['params_M']:>12.3f} {param_reduction:>+11.1f}%\")\n",
    "print(f\"{'mAP50':<20} {baseline_metrics['mAP50']:>12.4f} {dendritic_metrics['mAP50']:>12.4f} {map50_change:>+11.4f}\")\n",
    "print(f\"{'mAP50-95':<20} {baseline_metrics['mAP50-95']:>12.4f} {dendritic_metrics['mAP50-95']:>12.4f} {map_change:>+11.4f}\")\n",
    "print(f\"{'Inference (ms)':<20} {baseline_metrics['inference_ms']:>12.2f} {dendritic_metrics['inference_ms']:>12.2f} {speed_improvement:>+11.1f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüèÜ KEY RESULTS:\")\n",
    "print(f\"   üì¶ Parameter Reduction: {param_reduction:+.1f}%\")\n",
    "print(f\"   ‚ö° Speed Improvement: {speed_improvement:+.1f}%\")\n",
    "print(f\"   üéØ mAP50 Change: {map50_change:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VISUALIZATION ===\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Chart 1: mAP Comparison\n",
    "metrics_names = ['mAP50', 'mAP50-95']\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, [baseline_metrics['mAP50'], baseline_metrics['mAP50-95']], width, label='Baseline', color='steelblue')\n",
    "axes[0].bar(x + width/2, [dendritic_metrics['mAP50'], dendritic_metrics['mAP50-95']], width, label='Dendritic', color='coral')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics_names)\n",
    "axes[0].legend()\n",
    "\n",
    "# Chart 2: Parameters\n",
    "axes[1].bar(['Baseline', 'Dendritic'], [baseline_metrics['params_M'], dendritic_metrics['params_M']], color=['steelblue', 'coral'])\n",
    "axes[1].set_ylabel('Parameters (Millions)')\n",
    "axes[1].set_title('Model Size')\n",
    "for i, v in enumerate([baseline_metrics['params_M'], dendritic_metrics['params_M']]):\n",
    "    axes[1].text(i, v + 0.05, f'{v:.2f}M', ha='center')\n",
    "\n",
    "# Chart 3: Inference Speed\n",
    "axes[2].bar(['Baseline', 'Dendritic'], [baseline_metrics['inference_ms'], dendritic_metrics['inference_ms']], color=['steelblue', 'coral'])\n",
    "axes[2].set_ylabel('Inference Time (ms)')\n",
    "axes[2].set_title('Speed Comparison')\n",
    "for i, v in enumerate([baseline_metrics['inference_ms'], dendritic_metrics['inference_ms']]):\n",
    "    axes[2].text(i, v + 0.5, f'{v:.1f}ms', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Chart saved to 'comparison_chart.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVE RESULTS ===\n",
    "results = {\n",
    "    \"hackathon\": \"PyTorch Dendritic Optimization Hackathon\",\n",
    "    \"model\": \"YOLOv8n\",\n",
    "    \"dataset\": \"COCO128\",\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"baseline\": baseline_metrics,\n",
    "    \"dendritic\": dendritic_metrics,\n",
    "    \"improvements\": {\n",
    "        \"parameter_reduction_pct\": param_reduction,\n",
    "        \"speed_improvement_pct\": speed_improvement,\n",
    "        \"mAP50_change\": map50_change,\n",
    "        \"mAP50-95_change\": map_change\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('hackathon_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Results saved to 'hackathon_results.json'\")\n",
    "print(\"\\nüìã SUBMISSION READY!\")\n",
    "print(\"   1. Download comparison_chart.png and hackathon_results.json\")\n",
    "print(\"   2. Include in your PR to PerforatedAI/PerforatedAI\")\n",
    "print(\"   3. Submit PR link on Devpost\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
