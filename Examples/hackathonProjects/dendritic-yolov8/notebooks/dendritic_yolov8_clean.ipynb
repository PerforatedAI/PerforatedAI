{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Dendritic YOLOv8: Clean L4 GPU Version\n",
    "\n",
    "**FRESH START** - No kernel issues, optimized for L4 GPU\n",
    "\n",
    "## Quick Setup for L4 GPU:\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí L4 GPU ‚Üí Save**\n",
    "2. **Runtime ‚Üí Restart runtime** \n",
    "3. **Run cells in order** (each cell is designed to work independently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set W&B API Key FIRST\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"21942b7ed5b0ebedb98e928635acff2e972a99fc\"\n",
    "print(\"‚úÖ W&B API key set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN dependency install - no hanging\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "\n",
    "# Check if in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"In Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Installing for Colab L4 GPU...\")\n",
    "    !pip install -q ultralytics wandb matplotlib pandas seaborn\n",
    "    !pip install -q perforatedai==3.0.7\n",
    "    print(\"‚úÖ Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Local environment - ensure packages are installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and GPU check\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import wandb\n",
    "\n",
    "# GPU verification\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {memory_gb:.1f} GB\")\n",
    "    if 'L4' in gpu_name:\n",
    "        print(\"üéØ L4 GPU detected - perfect!\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"‚ö†Ô∏è No GPU - select L4 GPU in Runtime settings\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PerforatedAI import with fallback\n",
    "try:\n",
    "    from perforatedai import globals_perforatedai as GPA\n",
    "    from perforatedai import utils_perforatedai as UPA\n",
    "    PERFORATED_AI_AVAILABLE = True\n",
    "    print(\"‚úÖ PerforatedAI imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è PerforatedAI not available: {e}\")\n",
    "    PERFORATED_AI_AVAILABLE = False\n",
    "    \n",
    "    # Dummy classes\n",
    "    class DummyGPA:\n",
    "        class pc:\n",
    "            @staticmethod\n",
    "            def set_testing_dendrite_capacity(val): pass\n",
    "            @staticmethod\n",
    "            def set_verbose(val): pass\n",
    "            @staticmethod\n",
    "            def set_dendrite_update_mode(val): pass\n",
    "        class pai_tracker:\n",
    "            @staticmethod\n",
    "            def set_optimizer(opt): pass\n",
    "            @staticmethod\n",
    "            def set_scheduler(sched): pass\n",
    "            @staticmethod\n",
    "            def setup_optimizer(model, opt_args, sched_args): \n",
    "                import torch.optim as optim\n",
    "                return optim.Adam(model.parameters(), **opt_args), None\n",
    "            @staticmethod\n",
    "            def add_validation_score(score, model):\n",
    "                return model, False, True\n",
    "    \n",
    "    class DummyUPA:\n",
    "        @staticmethod\n",
    "        def initialize_pai(model, **kwargs):\n",
    "            return model\n",
    "    \n",
    "    GPA = DummyGPA()\n",
    "    UPA = DummyUPA()\n",
    "\n",
    "print(f\"PerforatedAI status: {PERFORATED_AI_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def measure_inference_speed(model, device, img_size=640, runs=50):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, img_size, img_size).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    for _ in range(runs):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    return (time.perf_counter() - start) / runs * 1000\n",
    "\n",
    "print(\"‚úÖ Helper functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B and load baseline model\n",
    "wandb.init(\n",
    "    project=\"Dendritic-YOLOv8-Clean\",\n",
    "    name=\"clean-run\",\n",
    "    tags=[\"l4-gpu\", \"clean-start\"],\n",
    "    config={\"model\": \"yolov8n\", \"dataset\": \"coco128\", \"epochs\": 3}\n",
    ")\n",
    "\n",
    "print(\"üöÄ Loading YOLOv8n...\")\n",
    "baseline_model = YOLO(\"yolov8n.pt\")\n",
    "baseline_model.model = baseline_model.model.to(device)\n",
    "\n",
    "# Get parameters\n",
    "total_params, trainable_params = count_parameters(baseline_model.model)\n",
    "print(f\"üìä Parameters: {total_params/1e6:.2f}M total, {trainable_params/1e6:.2f}M trainable\")\n",
    "\n",
    "wandb.log({\"baseline_params_M\": total_params/1e6})\n",
    "print(\"‚úÖ Baseline model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick baseline training (3 epochs)\n",
    "print(\"üöÄ Quick baseline training...\")\n",
    "\n",
    "try:\n",
    "    results = baseline_model.train(\n",
    "        data=\"coco128.yaml\",\n",
    "        epochs=3,  # Reduced for speed\n",
    "        imgsz=640,\n",
    "        batch=8,  # Conservative\n",
    "        device=device,\n",
    "        project=\"runs/clean\",\n",
    "        name=\"baseline\",\n",
    "        exist_ok=True,\n",
    "        verbose=False  # Reduce output\n",
    "    )\n",
    "    print(\"‚úÖ Baseline training complete\")\n",
    "    \n",
    "    # Quick validation\n",
    "    val_results = baseline_model.val(data=\"coco128.yaml\", device=device, verbose=False)\n",
    "    baseline_map50 = float(val_results.box.map50) if val_results.box.map50 else 0.0\n",
    "    print(f\"üìä Baseline mAP50: {baseline_map50:.4f}\")\n",
    "    \n",
    "    # Measure speed\n",
    "    baseline_speed = measure_inference_speed(baseline_model.model, device)\n",
    "    print(f\"‚ö° Baseline speed: {baseline_speed:.2f}ms\")\n",
    "    \n",
    "    baseline_metrics = {\n",
    "        \"mAP50\": baseline_map50,\n",
    "        \"params_M\": total_params/1e6,\n",
    "        \"speed_ms\": baseline_speed\n",
    "    }\n",
    "    \n",
    "    wandb.log({f\"baseline_{k}\": v for k, v in baseline_metrics.items()})\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    baseline_metrics = {\"mAP50\": 0.0, \"params_M\": total_params/1e6, \"speed_ms\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dendritic model\n",
    "print(\"üß† Setting up dendritic model...\")\n",
    "\n",
    "# Fresh model for dendritic optimization\n",
    "dendritic_yolo = YOLO(\"yolov8n.pt\")\n",
    "dendritic_model = dendritic_yolo.model.to(device)\n",
    "\n",
    "if PERFORATED_AI_AVAILABLE:\n",
    "    print(\"   Configuring PerforatedAI...\")\n",
    "    GPA.pc.set_testing_dendrite_capacity(False)\n",
    "    GPA.pc.set_verbose(True)\n",
    "    GPA.pc.set_dendrite_update_mode(True)\n",
    "    \n",
    "    try:\n",
    "        # Apply dendritic optimization\n",
    "        dendritic_model = UPA.initialize_pai(\n",
    "            dendritic_model,\n",
    "            doing_pai=True,\n",
    "            save_name=\"CleanDendriticYOLO\",\n",
    "            maximizing_score=True\n",
    "        )\n",
    "        print(\"‚úÖ Dendritic optimization applied\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è PerforatedAI optimization failed: {e}\")\n",
    "        print(\"   Continuing with standard model\")\n",
    "else:\n",
    "    print(\"   Using standard model (PerforatedAI not available)\")\n",
    "\n",
    "# Update YOLO wrapper\n",
    "dendritic_yolo.model = dendritic_model\n",
    "\n",
    "# Count parameters\n",
    "dendritic_total, dendritic_trainable = count_parameters(dendritic_model)\n",
    "print(f\"üìä Dendritic parameters: {dendritic_total/1e6:.2f}M total\")\n",
    "print(f\"üìä Parameter change: {((dendritic_total - total_params)/total_params)*100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick dendritic training\n",
    "print(\"üöÄ Quick dendritic training...\")\n",
    "\n",
    "try:\n",
    "    dendritic_results = dendritic_yolo.train(\n",
    "        data=\"coco128.yaml\",\n",
    "        epochs=3,  # Reduced for speed\n",
    "        imgsz=640,\n",
    "        batch=8,\n",
    "        device=device,\n",
    "        project=\"runs/clean\",\n",
    "        name=\"dendritic\",\n",
    "        exist_ok=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"‚úÖ Dendritic training complete\")\n",
    "    \n",
    "    # Quick validation\n",
    "    dendritic_val = dendritic_yolo.val(data=\"coco128.yaml\", device=device, verbose=False)\n",
    "    dendritic_map50 = float(dendritic_val.box.map50) if dendritic_val.box.map50 else 0.0\n",
    "    print(f\"üìä Dendritic mAP50: {dendritic_map50:.4f}\")\n",
    "    \n",
    "    # Add validation score to PerforatedAI tracker\n",
    "    if PERFORATED_AI_AVAILABLE:\n",
    "        try:\n",
    "            dendritic_model, restructured, complete = GPA.pai_tracker.add_validation_score(\n",
    "                dendritic_map50, dendritic_model\n",
    "            )\n",
    "            if restructured:\n",
    "                print(\"üîÑ Model restructured by PerforatedAI\")\n",
    "                dendritic_yolo.model = dendritic_model\n",
    "            if complete:\n",
    "                print(\"‚úÖ PerforatedAI optimization cycle complete\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è PerforatedAI validation scoring failed: {e}\")\n",
    "    \n",
    "    # Measure speed\n",
    "    dendritic_speed = measure_inference_speed(dendritic_yolo.model, device)\n",
    "    print(f\"‚ö° Dendritic speed: {dendritic_speed:.2f}ms\")\n",
    "    \n",
    "    dendritic_metrics = {\n",
    "        \"mAP50\": dendritic_map50,\n",
    "        \"params_M\": dendritic_total/1e6,\n",
    "        \"speed_ms\": dendritic_speed\n",
    "    }\n",
    "    \n",
    "    wandb.log({f\"dendritic_{k}\": v for k, v in dendritic_metrics.items()})\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dendritic training failed: {e}\")\n",
    "    dendritic_metrics = {\"mAP50\": 0.0, \"params_M\": dendritic_total/1e6, \"speed_ms\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison and visualization\n",
    "print(\"üìä Generating results...\")\n",
    "\n",
    "# Calculate improvements\n",
    "improvements = {}\n",
    "if baseline_metrics['params_M'] > 0:\n",
    "    improvements['param_reduction_pct'] = ((baseline_metrics['params_M'] - dendritic_metrics['params_M']) / baseline_metrics['params_M']) * 100\n",
    "\n",
    "improvements['mAP50_change'] = dendritic_metrics['mAP50'] - baseline_metrics['mAP50']\n",
    "\n",
    "if baseline_metrics['speed_ms'] > 0:\n",
    "    improvements['speed_change_pct'] = ((baseline_metrics['speed_ms'] - dendritic_metrics['speed_ms']) / baseline_metrics['speed_ms']) * 100\n",
    "\n",
    "# Create comparison chart\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# mAP comparison\n",
    "map_vals = [baseline_metrics['mAP50'], dendritic_metrics['mAP50']]\n",
    "axes[0].bar(['Baseline', 'Dendritic'], map_vals, color=['steelblue', 'coral'])\n",
    "axes[0].set_ylabel('mAP50')\n",
    "axes[0].set_title('mAP50 Comparison')\n",
    "\n",
    "# Parameters comparison\n",
    "param_vals = [baseline_metrics['params_M'], dendritic_metrics['params_M']]\n",
    "axes[1].bar(['Baseline', 'Dendritic'], param_vals, color=['steelblue', 'coral'])\n",
    "axes[1].set_ylabel('Parameters (M)')\n",
    "axes[1].set_title('Model Size')\n",
    "\n",
    "# Speed comparison\n",
    "speed_vals = [baseline_metrics['speed_ms'], dendritic_metrics['speed_ms']]\n",
    "axes[2].bar(['Baseline', 'Dendritic'], speed_vals, color=['steelblue', 'coral'])\n",
    "axes[2].set_ylabel('Inference Time (ms)')\n",
    "axes[2].set_title('Inference Speed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clean_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ CLEAN DENDRITIC YOLOV8 RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Baseline:  mAP50={baseline_metrics['mAP50']:.4f}, Params={baseline_metrics['params_M']:.2f}M, Speed={baseline_metrics['speed_ms']:.1f}ms\")\n",
    "print(f\"üìä Dendritic: mAP50={dendritic_metrics['mAP50']:.4f}, Params={dendritic_metrics['params_M']:.2f}M, Speed={dendritic_metrics['speed_ms']:.1f}ms\")\n",
    "print(f\"\\nüìà Improvements:\")\n",
    "for key, value in improvements.items():\n",
    "    print(f\"   {key}: {value:+.2f}{'%' if 'pct' in key else ''}\")\n",
    "print(f\"\\nüîß PerforatedAI: {'‚úÖ Active' if PERFORATED_AI_AVAILABLE else '‚ùå Not Available'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "results = {\n",
    "    \"project\": \"Dendritic YOLOv8 - Clean Implementation\",\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"environment\": {\n",
    "        \"device\": device,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"none\",\n",
    "        \"pytorch_version\": torch.__version__,\n",
    "        \"perforated_ai_available\": PERFORATED_AI_AVAILABLE\n",
    "    },\n",
    "    \"baseline\": baseline_metrics,\n",
    "    \"dendritic\": dendritic_metrics,\n",
    "    \"improvements\": improvements,\n",
    "    \"success\": True\n",
    "}\n",
    "\n",
    "with open('clean_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Results saved to 'clean_results.json'\")\n",
    "print(\"‚úÖ Chart saved to 'clean_comparison.png'\")\n",
    "print(\"\\nüéØ Clean execution complete!\")\n",
    "print(\"üìÅ Download files: clean_results.json, clean_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}