# -*- coding: utf-8 -*-
"""Dendrte 2 Improved.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1utELeNL3ogxjge8TYqiTmVlEQ-1mxtaE
"""

import os
os.environ["ACCELERATE_MIXED_PRECISION"] = "no"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"

# ============================================================
# SYSTEM SETUP
# ============================================================

!nvidia-smi
!pip install -q transformers accelerate bitsandbytes datasets scikit-learn pandas matplotlib seaborn peft
!git clone https://github.com/PerforatedAI/PerforatedAI.git || true
!pip install -q -e ./PerforatedAI

# ============================================================
# IMPORTS
# ============================================================

import sys, time, torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
)
from datasets import load_dataset
from sklearn.metrics import accuracy_score, f1_score
from sklearn.utils.class_weight import compute_class_weight
from peft import LoraConfig, get_peft_model

sys.path.insert(0, "./PerforatedAI")
from perforatedai import utils_perforatedai as UPA
from perforatedai import globals_perforatedai as GPA

GPA.pc.set_unwrapped_modules_confirmed(True)
sns.set_style("whitegrid")

os.makedirs("results", exist_ok=True)

# ============================================================
# DATASET
# ============================================================

dataset = load_dataset("gtfintechlab/financial_phrasebank_sentences_allagree", "5768")
spl = dataset["train"].train_test_split(test_size=0.3, seed=42)
tv = spl["test"].train_test_split(test_size=0.5, seed=42)

data = {"train": spl["train"], "val": tv["train"], "test": tv["test"]}

# ============================================================
# TOKENIZER
# ============================================================

MODEL_NAME = "microsoft/phi-2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

def tokenize(batch):
    return tokenizer(batch["sentence"], truncation=True, padding="max_length", max_length=192)

datasets_tok = {k: v.map(tokenize, batched=True).remove_columns(["sentence"]) for k, v in data.items()}
for ds in datasets_tok.values():
    ds.set_format("torch")

# ============================================================
# METRICS & LOSS
# ============================================================

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {"accuracy": accuracy_score(labels, preds), "f1": f1_score(labels, preds, average="weighted")}

cw = compute_class_weight("balanced", classes=np.array([0,1,2]), y=np.array(data["train"]["label"]))
cw = torch.tensor(cw, dtype=torch.float)

class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs["labels"]
        outputs = model(**inputs)
        logits = outputs.logits.float()
        loss = nn.CrossEntropyLoss(weight=cw.to(logits.device))(logits, labels)
        return (loss, outputs) if return_outputs else loss

# ============================================================
# QUANTIZATION
# ============================================================

bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4",
                                bnb_4bit_compute_dtype=torch.float16)

# ============================================================
# BASELINE TRAINING
# ============================================================

baseline = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=3, quantization_config=bnb_config,
    device_map="auto", pad_token_id=tokenizer.pad_token_id
)
baseline.config.use_cache = False
baseline = get_peft_model(baseline, LoraConfig(
    r=32, lora_alpha=64, target_modules=["q_proj","v_proj"],
    lora_dropout=0.05, task_type="SEQ_CLS"
))

baseline_logs = []

baseline_trainer = WeightedTrainer(
    model=baseline,
    args=TrainingArguments(
        output_dir="./baseline", eval_strategy="epoch",
        learning_rate=1e-5, per_device_train_batch_size=4,
        per_device_eval_batch_size=4, num_train_epochs=3,
        fp16=False, bf16=False, report_to="none"
    ),
    train_dataset=datasets_tok["train"],
    eval_dataset=datasets_tok["val"],
    compute_metrics=compute_metrics,
)

baseline_trainer.train()
baseline_metrics = baseline_trainer.evaluate(datasets_tok["test"])
baseline_logs = baseline_trainer.state.log_history

del baseline_trainer, baseline
torch.cuda.empty_cache()

# ============================================================
# DENDRITIC TRAINING
# ============================================================

dmodel = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=3, quantization_config=bnb_config,
    device_map="auto", pad_token_id=tokenizer.pad_token_id
)
dmodel.config.use_cache = False
dmodel = UPA.initialize_pai(dmodel)
dmodel.score.set_this_output_dimensions([0,192,3])

dmodel = get_peft_model(dmodel, LoraConfig(
    r=32, lora_alpha=64, target_modules=["q_proj","v_proj"],
    lora_dropout=0.05, task_type="SEQ_CLS"
))

dendritic_logs = []

d_trainer = WeightedTrainer(
    model=dmodel,
    args=TrainingArguments(
        output_dir="./dendritic", eval_strategy="epoch",
        learning_rate=8e-6, per_device_train_batch_size=4,
        per_device_eval_batch_size=4, num_train_epochs=6,
        warmup_steps=300, fp16=False, bf16=False, report_to="none"
    ),
    train_dataset=datasets_tok["train"],
    eval_dataset=datasets_tok["val"],
    compute_metrics=compute_metrics,
)

d_trainer.train()
dendritic_metrics = d_trainer.evaluate(datasets_tok["test"])
dendritic_logs = d_trainer.state.log_history

# ============================================================
# SAVE RESULTS CSV
# ============================================================

results_df = pd.DataFrame([
    {"Model":"Baseline","Accuracy":baseline_metrics["eval_accuracy"],
     "F1":baseline_metrics["eval_f1"]},
    {"Model":"Dendritic","Accuracy":dendritic_metrics["eval_accuracy"],
     "F1":dendritic_metrics["eval_f1"]}
])
results_df.to_csv("Results.csv", index=False)

# ============================================================
# RAW TRAINING GRAPH (REQUIRED)
# ============================================================

def plot_raw(logs, label, color):
    epochs, acc = [], []
    for l in logs:
        if "eval_accuracy" in l:
            epochs.append(l["epoch"])
            acc.append(l["eval_accuracy"])
    plt.plot(epochs, acc, marker="o", label=label, color=color)

plt.figure(figsize=(10,6), dpi=300)
plot_raw(baseline_logs, "Baseline", "blue")
plot_raw(dendritic_logs, "Dendritic", "orange")
plt.xlabel("Epoch")
plt.ylabel("Validation Accuracy")
plt.title("Raw Training Accuracy Curves")
plt.legend()
plt.tight_layout()
plt.savefig("Dendrite_vs_Baseline_Training.png")
plt.close()

# ============================================================
# CLEAN ACCURACY IMPROVEMENT GRAPH (OPTIONAL)
# ============================================================

plt.figure(figsize=(6,5), dpi=300)
sns.barplot(data=results_df, x="Model", y="Accuracy", palette="viridis")
plt.title("Accuracy Improvement")
for i, v in enumerate(results_df["Accuracy"]):
    plt.text(i, v+0.005, f"{v:.3f}", ha="center")
plt.tight_layout()
plt.savefig("Accuracy_Improvement.png")
plt.close()

print("\nArtifacts generated:")
print("- Results.csv")
print("- Accuracy_Improvement.png")
print("- Dendrite_vs_Baseline_Training.png")